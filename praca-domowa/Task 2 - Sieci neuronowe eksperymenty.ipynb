{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730744da-5501-418e-a590-5aa58d52d5e2",
   "metadata": {},
   "source": [
    "### zadanie 2 \n",
    "W tym zadaniu poeksperymentujemy z różnymi architekturami sieci neurnonowej - oraz różnymi optimizerami - zobaczymy jaki to ma wpływ na postęp uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c460a80-36e6-45c7-97df-4bcfe34b1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj tę komórkę - zawiera funkcje potrzebne do wczytania / przetwarzania danych \n",
    "\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import random \n",
    "def shuffle_data(x, y):\n",
    "    c = list(zip(x, y))\n",
    "    random.shuffle(c)\n",
    "    return zip(*c)\n",
    "\n",
    "filename = [\n",
    "    [\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "    [\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "    [\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "    [\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def preprocess(x):\n",
    "    return [y.flatten()  for y in x]\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    mean_val = np.mean(x)\n",
    "    stdev_val = np.std(x)\n",
    "    return (x - mean_val) / stdev_val\n",
    "\n",
    "def binarize(y):\n",
    "    return [int(elem == 5) for elem in y]\n",
    "         \n",
    "\n",
    "def load(n = 5000):\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return (\n",
    "        normalize(preprocess(mnist[\"training_images\"][:n]  / 255.)), \n",
    "        [x[0] for x in preprocess(mnist[\"training_labels\"][:n])], \n",
    "        normalize(preprocess(mnist[\"test_images\"][:n]  / 255.)), \n",
    "        [x[0] for x in preprocess(mnist[\"test_labels\"][:n])]\n",
    "    )\n",
    "\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82354b34-9f9f-47cd-9673-4dca3827f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytaj tę komórkę. \n",
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train, y_train = zip(*[x for x in list(zip(x_train, y_train)) if x[1] == 5 or (random.random() < 0.12)])\n",
    "x_test, y_test = zip(*[x for x in list(zip(x_test, y_test)) if x[1] == 5 or (random.random() < 0.12)])\n",
    "\n",
    "y_train_bin = binarize(y_train)\n",
    "y_test_bin = binarize(y_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499f28f-128d-4d1f-9417-d7e8128d1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 50),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(50,15),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(15, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38666c32-eeb2-4930-bb7e-941fc799a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop - pętla trenująca została zamknięta w funkcję train \n",
    "\n",
    "from tqdm import tqdm\n",
    "def training_loop(model, optimizer, x_train, y_train, batch_size = 128, epochs=50):\n",
    "    losses = []\n",
    "    bce_loss = nn.BCELoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        x, y = shuffle_data(x_train,y_train_bin)\n",
    "        current_index = 0 \n",
    "        while current_index < len(x_train):\n",
    "            batch_x = x[current_index:(current_index + batch_size)]\n",
    "            batch_y = y[current_index: (current_index + batch_size)] # \n",
    "            \n",
    "            tensor_batch_x = torch.Tensor(batch_x)\n",
    "            tensor_batch_y = torch.Tensor(batch_y).reshape(-1, 1)\n",
    "            \n",
    "            pred = model.forward(tensor_batch_x)\n",
    "            loss = bce_loss(pred, tensor_batch_y)\n",
    "\n",
    "                \n",
    "            # Backpropagation\n",
    "            loss.backward() # Liczenie gradientu wag modelu\n",
    "            optimizer.step() # adam oblicza nowe parametry sieci \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            current_index += batch_size\n",
    "            losses.append(float(loss.detach().numpy()))\n",
    "            \n",
    "    return model, losses  # funkcja zwraca dwie wartości: nauczony model oraz listę losses(błędów) w każdej iteracji  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dde24-b7ec-4341-a7df-c5cd9b0e063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.\n",
    "model = MultiLayerPerceptron()\n",
    "optimizer = None # TODO: wypróbuj optimizer Adam : torch.optim.Adam\n",
    "trained_model, losses_adam = training_loop(model=model, optimizer = optimizer, x_train=x_train, y_train=y_train_bin, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faac812-6eb2-4281-b590-8e23d2fb82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.\n",
    "model = MultiLayerPerceptron()\n",
    "optimizer = None  # TODO: wypróbuj optimzier SGD torch.optim.SGD poeksperymentuj z parametrem lr= [wypróbuj wartości 0.01, 0.05, 0.1, 0.2]\n",
    "trained_model, losses_sgd = training_loop(model=model, optimizer = optimizer, x_train=x_train, y_train=y_train_bin, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90466af2-04a8-409f-ab05-95517e015274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.\n",
    "model = MultiLayerPerceptron()\n",
    "optimizer = None # TODO: wypróbuj AdamW torch.optim.AdamW \n",
    "trained_model,losses_adamw = training_loop(model=model, optimizer = optimizer, x_train=x_train, y_train=y_train_bin, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317d2d7-c6e5-47d9-91d8-60f942c3ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task4. \n",
    "# TODO: Pokaż jak loss/error wszystkich trzech optimizerów zmienia się w czasie\n",
    "\n",
    "\n",
    "\n",
    "# pd.Series(losses_adam).plot(label='adam')\n",
    "# pd.Series(losses_adamw).plot(label='adamw')\n",
    "# pd.Series(losses_sgd).plot(label='sgd')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcfcb8-a9f1-4d73-807d-f59e1c212b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Poeksperymentuj z różnymi parami sieci neuronowych i zobacz jak zmienia się loss po zmianie architektury sieci oraz funkcji aktywacji \n",
    "\n",
    "# Twój oryginalny perceptron \n",
    "class MultiLayerPerceptronA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 50),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(50,15),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(15, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return self.mlp(x)\n",
    "\n",
    "# Perceptron z którym go porównasz \n",
    "class MultiLayerPerceptronB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(784, 50), # TODO: Poeksperymentuj z różną ilością neuronów \n",
    "            nn.Sigmoid(), # TODO: Zamień aktywacje na ReLU \n",
    "            nn.Linear(50, 15), # TODO: Poeksperymentuj z różną ilością neuronów \n",
    "            nn.Sigmoid(),  # TODO: Zamień aktywacje na ReLU \n",
    "            nn.Linear(15, 1), # TODO: Poeksperymentuj z różną ilością neuronów \n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f89e9-3894-4ecd-b9fa-c0a1cfd5131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptronA()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "losses_a = training_loop(model=model, optimizer = optimizer, x_train=x_train, y_train=y_train_bin, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e987d2-1128-4f53-babd-bb3ef11292b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptronB()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "losses_b = training_loop(model=model, optimizer = optimizer, x_train=x_train, y_train=y_train_bin, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a6f27-21bc-487b-b95a-573257f5f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proces uczenia dla powyższych par A, B \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "pd.Series(losses_a).plot(label='A')\n",
    "pd.Series(losses_b).plot(label='B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bd029b-d4dd-4c30-a014-79558e95b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. \n",
    "# TODO (dla chętnych): Zaimplementuj funkcje które wyliczają 2 wartości:\n",
    "# https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "# True Positive rate \n",
    "# False Positive rate  \n",
    "\n",
    "# Są to kolejne wazne metryki których używamy do mierzenia jakości rozwiązania problemu klasyfikacji binarnej \n",
    "# Spróbuj zaimplementować je i zmierzyć te wartości dla różnych thresholdów któregoś z wyuczonych modeli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92346677-c6ed-4453-a79c-1f669df234fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
