{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5f35a3-26a3-4e49-891b-2487fd6e39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm \n",
    "\n",
    "CHANGE_THIS_VALUE = None \n",
    "\n",
    "def create_bbox_coords(bbox):\n",
    "    \n",
    "    xmin = float(bbox.find('xmin').text)\n",
    "    ymin = float(bbox.find('ymin').text)\n",
    "    xmax = float(bbox.find('xmax').text)\n",
    "    ymax = float(bbox.find('ymax').text)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def create_mask(plasmodium_img, bbox):\n",
    "    xmin, ymin, xmax, ymax = create_bbox_coords(bbox)\n",
    "    mask = np.zeros((plasmodium_img.size[1], plasmodium_img.size[0]), dtype=np.uint8)\n",
    "    mask[int(ymin):int(ymax), int(xmin):int(xmax)] = 1 \n",
    "    return mask \n",
    "    \n",
    "class MalariaPlasmodiumDataset(torch.utils.data.Dataset):\n",
    "    # Będziemy czytać pliki jpg i odpowiadające im pliki XML \n",
    "    # z katalogu directory_root \n",
    "    # Podamy też transformacje jakie chcemy przeprowadzać na zwracanych wartościach \n",
    "    \n",
    "    def __init__(self, directory_root, images_transforms=None):\n",
    "\n",
    "        # Przypisujemy parametetry konstruktora do self \n",
    "        # Chcemy aby nasz przyszły obiekt wiedzial o tym gdzie szukać plików oraz \n",
    "        # jakie transformacje wykonywać na przeczytanych JPG \n",
    "        self.directory_root = directory_root        \n",
    "        self.images_transforms = images_transforms\n",
    "\n",
    "        # Listujemy wszystkie pliki które mają rozszerzenie \"JPG\" \n",
    "        all_image_files = sorted([img for img in os.listdir(directory_root) if img.endswith(\".jpg\")])\n",
    "        \n",
    "\n",
    "        # wśród zdjęć w naszym datasecie są takie, na których nie znaleziono zarodźca \n",
    "        # usuwamy je z datasetu - tzn zapisujemy do self.imgs_with_plasmodium tylko \n",
    "        self.imgs_with_plasmodium = []\n",
    "        for img_file in all_image_files:\n",
    "            xml_file = os.path.join(self.directory_root, img_file.replace(\".jpg\", \".xml\"))\n",
    "            tree = ET.parse(xml_file)\n",
    "            # wykrycie zarodźca na zdjęciu jpg jest równoważne z istnieniem taga \"object\" w XML - jeśli tylko znajdziemy takowy \n",
    "            # kwalifikujemy zdjęcie jako dobre do naszego wejściowego datasetu i dodajemy nazwę pliku do self.imgs_with_plasmodium  \n",
    "            if tree.findall('object'): \n",
    "                self.imgs_with_plasmodium.append(img_file)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # \"magiczna\" metoda __getitem__ jest wykorzystywana kiedy chcemy aby nasz obiekt był dostępny poprzez operator [int] \n",
    "        # podobnie jak lista czy dict \n",
    "        single_plasmodium_img_path = self.get_single_plasmodium_path(idx)\n",
    "        single_annotation_file_path = single_plasmodium_img_path.replace(\".jpg\", \".xml\")\n",
    "        plasmodium_img = Image.open(single_plasmodium_img_path).convert(\"RGB\") \n",
    "        \n",
    "        # read xml file\n",
    "        annotations = ET.parse(single_annotation_file_path)\n",
    "        boxes = []\n",
    "        masks = []        \n",
    "        \n",
    "        for detected_plasmodium in annotations.findall('object'):            \n",
    "            bbox = detected_plasmodium.find('bndbox')\n",
    "            boxes.append(\n",
    "                create_bbox_coords(bbox)\n",
    "            )\n",
    "        \n",
    "            masks.append(\n",
    "                create_mask(\n",
    "                    plasmodium_img, bbox\n",
    "                )\n",
    "            )\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)  \n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)                            \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])            \n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        \n",
    "        if self.images_transforms is not None:\n",
    "            transformed_plasmodium_img = self.images_transforms(plasmodium_img)\n",
    "        else:\n",
    "            transformed_plasmodium_img = plasmodium_img            \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"masks\"] = masks\n",
    "        \n",
    "        return transformed_plasmodium_img, target\n",
    "\n",
    "    def get_single_plasmodium_path(self, idx):\n",
    "        single_plasmodium_img_path = os.path.join(self.directory_root, self.imgs_with_plasmodium[idx])\n",
    "        return single_plasmodium_img_path\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # magiczna metoda __len__ jest używana gdy na instancji wykonujemy len() \n",
    "        return len(self.imgs_with_plasmodium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574f786-5568-49e1-ad84-a09d1221930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bounding_boxes(image_path, bboxes, scores=None, color=(255, 0, 0), return_pt = False):    \n",
    "    img_pil = Image.open(image_path).convert(\"RGBA\")\n",
    "    new = Image.new('RGBA', img_pil.size, (255, 255, 255, 0))\n",
    "    draw =ImageDraw.Draw(new)\n",
    "\n",
    "    for i, box in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = box        \n",
    "        if scores is not None:          \n",
    "            alpha = int(255 * scores[i])  # Convert score to an alpha value.                      \n",
    "            color_with_alpha = color + (alpha,)\n",
    "        else:       \n",
    "            color_with_alpha = color + (255,)\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=color_with_alpha, width=2)\n",
    "\n",
    "    out = Image.alpha_composite(img_pil, new).convert(\"RGB\")\n",
    "    return T.ToTensor()(out) if return_pt else out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4de2f6-aae7-43c3-85d8-f6e6448f771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "num_classes = 2  # 1 zarodziec + tło\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939eb25e-8f35-4cb9-bbde-b9deacdf487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a00ed-d034-4c85-9ec1-209afdb3a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"test_set_indices.json\", \"r\") as f:\n",
    "    test_indices = json.load(f)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "test_metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds = [0.5])\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(test_indices):\n",
    "        image, target = dataset[i]        \n",
    "        target = {k: v.to(device) for k, v in target.items()}    \n",
    "        output = model([image.to(device)])        \n",
    "        \n",
    "        test_metric.update(output, [target])    \n",
    "        torch.cuda.empty_cache()\n",
    "print(test_metric.compute()['map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfdc89-6b81-4025-ae33-7c9a2c61ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random \n",
    "test_index = random.choice(test_indices)\n",
    "with torch.no_grad():    \n",
    "    image, target = dataset[test_index]        \n",
    "    target = {k: v.to(device) for k, v in target.items()}\n",
    "    output = model([image.to(device)])\n",
    "    output[0]['scores'][output[0]['scores'] < 0.95] = 0\n",
    "    test_metric.update(output, [target])    \n",
    "    torch.cuda.empty_cache()\n",
    "    bboxes_true = target['boxes']\n",
    "    bboxes_predicted = output[0]['boxes']\n",
    "    scores = output[0]['scores']\n",
    "    img_id = target['image_id']\n",
    "    img = dataset.get_single_plasmodium_path(target['image_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9720e195-9975-4a41-9f85-894270256fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bounding_boxes(img, bboxes_predicted, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef365bd-e1aa-4482-90ca-821207c89e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bounding_boxes(img, bboxes_predicted, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad8bb67-ee09-40ee-a776-ad727c2f4de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10456d8e-888c-4d93-9d75-046d2ebb9e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cd772-3147-4a32-8d1a-85390f48b19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
